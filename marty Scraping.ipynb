{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup #use beautifulsoup to scrape data\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "url = \"http://millercenter.org/the-presidency/presidential-speeches\"\n",
    "\n",
    "browser = webdriver.Chrome() #change this line depending on your system\n",
    "browser.get(url) #go to the website \n",
    "\n",
    "\n",
    "\n",
    "for i in range(43,44):\n",
    "    x = i\n",
    "    absolutepath_president = f'/html/body/div[2]/div/main/div[2]/div/div[2]/article/div/div[2]/div/div/div/form/div[3]/fieldset/div/div/div/div[{x}]/label'\n",
    "    president1 = browser.find_element_by_xpath(absolutepath_president)\n",
    "    president1.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "# president = browser.find_element_by_xpath(\n",
    "#     '/html/body/div[2]/div/main/div[2]/div/div[2]/article/div/div[2]/div/div/div/form/div[3]/fieldset/div/div/div/div[42]/label')\n",
    "# president.click()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SCROLL_PAUSE_TIME = 0.8\n",
    "\n",
    "\n",
    "#slight changes to be made!!\n",
    "\n",
    "# Get scroll height\n",
    "last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait to load page\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "soup = BeautifulSoup(browser.page_source, 'lxml')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3 \n",
    "import csv\n",
    "\n",
    "\n",
    "link_str = []\n",
    "\n",
    "for link in soup.findAll('a', href=True):\n",
    "    if link != None:\n",
    "        link_str.append(link['href'])\n",
    "\n",
    "# now I want the link strings containting /the-presidency/\n",
    "new_link_str = []\n",
    "\n",
    "for element in link_str:\n",
    "    if element.find(\"/presidential-speeches/\") >= 0: \n",
    "        new_link_str.append(element)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# print(new_link_str)\n",
    "\n",
    "obama = dict()\n",
    "#I am writing this one specificly for obama as a test\n",
    "\n",
    "#this dictionary will have the date and name of the speech as key\n",
    "#and the speech itself as value\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "list2=[]\n",
    "for element in new_link_str:\n",
    "    url = \"http://millercenter.org\" + element\n",
    "#     print(element)\n",
    "    response = http.request('GET', url)\n",
    "    soup_2 = BeautifulSoup(response.data, 'lxml')\n",
    "    list2.append(soup_2.findAll('div', {\"class\":\"transcript-inner\"}))\n",
    "\n",
    "\n",
    "#converting into string due to weird beautifulsoup output    \n",
    "list3=[]\n",
    "for i in range(len(list2)):\n",
    "    list3.append(str(list2[i]))\n",
    "\n",
    "# print(list3[1])\n",
    "\n",
    "#this will be used to clean the text from html tags\n",
    "def tag_cleanr(text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext\n",
    "\n",
    "print(new_link_str[1])\n",
    "\n",
    "pattern = r'\\w+-\\d+-\\d{4}'\n",
    "match = re.search(pattern, new_link_str[1])\n",
    "print(match)\n",
    "\n",
    "\n",
    "s = tag_cleanr(list3[1])\n",
    "\n",
    "#DATAFRAME\n",
    "\n",
    "# with open('obamaspeeches.csv', 'w') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(join(list2))\n",
    "    \n",
    "    \n",
    "#     obama.update({element:soup_2.findAll('div', {\"class\":\"transcript-inner\"})})\n",
    "#print(obama)\n",
    "\n",
    "# with open('obamaspeeches.csv','wb') as f:\n",
    "#     w = csv.writer(f)\n",
    "#     w.writerows(obama.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a function to map from a date to a President"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import Piecewise\n",
    "import numpy\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "from sympy.abc import x\n",
    "import datetime\n",
    "import re\n",
    "def pres_from_date(date_):\n",
    "    p = lambdify(x, Piecewise(('Theodore_Roosevelt', ((x >= date(1901, 9, 14).toordinal()) & (x < date(1909, 3, 4).toordinal()))),\n",
    "                              ('William_Howard_Taft', ((x >= date(1909, 3, 4).toordinal()) & (x < date(1913, 3, 4).toordinal()))),\n",
    "                              ('Woodrow_Wilson', ((x >= date(1913, 3, 4).toordinal()) & (x < date(1921, 3, 4).toordinal()))),\n",
    "                              ('Warren_G_Harding', ((x >= date(1921, 3, 4).toordinal()) & (x < date(1923, 8, 3).toordinal()))),\n",
    "                              ('Calvin_Coolidge', ((x >= date(1923, 8, 3).toordinal()) & (x < date(1929, 3, 4).toordinal()))),\n",
    "                              ('Herbert_Hoover', ((x >= date(1929, 3, 4).toordinal()) & (x < date(1933, 3, 4).toordinal()))),\n",
    "                              ('Franklin_D_Roosevelt', ((x >= date(1933, 3, 4).toordinal()) & (x < date(1945, 4, 12).toordinal()))),\n",
    "                              ('Harry_S_Truman', ((x >= date(1945, 4, 12).toordinal()) & (x < date(1953, 1, 20).toordinal()))),\n",
    "                              ('Dwight_D_Eisenhower', ((x >= date(1953, 1, 20).toordinal()) & (x < date(1961, 1, 20).toordinal()))),\n",
    "                              ('John_F_Kennedy', ((x >= date(1961, 1, 20).toordinal()) & (x < date(1963, 11, 22).toordinal()))),\n",
    "                              ('Lyndon_B_Johnson', ((x >= date(1963, 11, 22).toordinal()) & (x < date(1969, 1, 20).toordinal()))),\n",
    "                              ('Richard_M_Nixon', ((x >= date(1969, 1, 20).toordinal()) & (x < date(1974, 8, 9).toordinal()))),\n",
    "                              ('Gerald_R_Ford', ((x >= date(1974, 8, 9).toordinal()) & (x < date(1977, 1, 20).toordinal()))),\n",
    "                              ('Jimmy_Carter', ((x >= date(1977, 1, 20).toordinal()) & (x < date(1981, 1, 20).toordinal()))),\n",
    "                              ('Ronald_Reagan', ((x >= date(1981, 1, 20).toordinal()) & (x < date(1989, 1, 20).toordinal()))),\n",
    "                              ('George_H_W_Bush', ((x >= date(1989, 1, 20).toordinal()) & (x < date(1993, 1, 20).toordinal()))),\n",
    "                              ('William_J_Clinton', ((x >= date(1993, 1, 20).toordinal()) & (x < date(2001, 1, 20).toordinal()))),\n",
    "                              ('George_W_Bush', ((x >= date(2001, 1, 20).toordinal()) & (x < date(2009, 1, 20).toordinal()))),\n",
    "                              ('Barack_Obama', ((x >= date(2009, 1, 20).toordinal()) & (x < date(2017, 1, 20).toordinal()))),\n",
    "                              ('Donald_J_Trump', (x >= date(2017, 1, 20).toordinal()))\n",
    "                              ),\n",
    "                              \"numpy\")\n",
    "    return re.sub('_', \" \", str(p(date_.toordinal())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up data frame\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('Dow_Jones_data_since_1900.csv')\n",
    "df2 = pd.read_csv('Nasdaq_since_1971.csv')\n",
    "df3 = pd.read_csv('S&P_data_since_1950.csv')\n",
    "df1 = df1[df1.Dow_Change != 0]\n",
    "df1 = df1.drop(columns = 'Close')\n",
    "df2 = df2.drop(columns = 'Close')\n",
    "df3 = df3.drop(columns = 'Close')\n",
    "fin_data = df1.merge(df2, how = \"left\")\n",
    "fin_data = fin_data.merge(df3, how = \"left\")\n",
    "\n",
    "## This function should work if there are no instances where two speeches happened on consecutive days where\n",
    "def add_speeches_to_fin_data(df, fin_data):\n",
    "    for i in len(df.Date):\n",
    "        df.Date[i] = df.Date[i] + datetime.timedelta(days = 1)\n",
    "    final = fin_data.merge(df, how = \"left\")\n",
    "    for i in list(pd.isnull(fin_data.Dow_Change).nonzero()[0]):\n",
    "        final.Date[i + 1] = final.Date[i]\n",
    "        final.Transcript[i + 1] = final.Date[i]\n",
    "    final = final[pd.notna(final.Dow_Change)]        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
